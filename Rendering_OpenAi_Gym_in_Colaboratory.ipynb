{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rendering OpenAi Gym in Colaboratory.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shankarfierce/MSDS_462_CV/blob/master/Rendering_OpenAi_Gym_in_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsSGgH2Y0apl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#          _____                _____                    _____                    _____                    _____                    _____          \n",
        "#         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n",
        "#        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n",
        "#       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n",
        "#      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n",
        "#     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n",
        "#    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n",
        "#    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n",
        "#  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n",
        "# /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n",
        "#/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n",
        "#\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n",
        "# \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n",
        "#  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n",
        "#   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n",
        "#    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n",
        "#     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n",
        "#      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n",
        "#       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n",
        "#        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n",
        "#         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE",
        "colab_type": "text"
      },
      "source": [
        "Pacman Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M",
        "colab_type": "text"
      },
      "source": [
        "# Pacman!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nj5sjsk15IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ-I1wvPUnL1",
        "colab_type": "text"
      },
      "source": [
        "The below model building code is referenced and sourced from \n",
        "\n",
        "https://github.com/moduIo/Deep-Q-network/blob/c24ec378a6157d6500decf6457945c694f7ea038/DDQN.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ1RYKDHTyhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y5tACstT2E_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DDQN_Agent:\n",
        "    #\n",
        "    # Initializes attributes and constructs CNN model and target_model\n",
        "    #\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.99           # Discount rate\n",
        "        self.epsilon = 1.0          # Exploration rate\n",
        "        self.epsilon_min = 0.1      # Minimal exploration rate (epsilon-greedy)\n",
        "        self.epsilon_decay = 0.995  # Decay rate for epsilon\n",
        "        self.update_rate = 10000    # Number of steps until updating the target network\n",
        "        \n",
        "        # Construct DQN models\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.model.summary()\n",
        "\n",
        "    #\n",
        "    # Constructs CNN\n",
        "    #\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        \n",
        "        # Conv Layers\n",
        "        model.add(Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n",
        "        model.add(Activation('relu'))\n",
        "        \n",
        "        model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        \n",
        "        model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Flatten())\n",
        "\n",
        "        # FC Layers\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        \n",
        "        model.compile(loss='mse', optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=None, decay=0.0)\n",
        ")\n",
        "        return model\n",
        "\n",
        "    #\n",
        "    # Stores experience in replay memory\n",
        "    #\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    #\n",
        "    # Chooses action based on epsilon-greedy policy\n",
        "    #\n",
        "    def act(self, state):\n",
        "        # Random exploration\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        \n",
        "        act_values = self.model.predict(state)\n",
        "        \n",
        "        return np.argmax(act_values[0])  # Returns action using policy\n",
        "\n",
        "    #\n",
        "    # Trains the model using randomly selected experiences in the replay memory\n",
        "    #\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        \n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            \n",
        "            if not done:\n",
        "                max_action = np.argmax(self.model.predict(next_state)[0])\n",
        "                target = (reward + self.gamma * self.target_model.predict(next_state)[0][max_action])\n",
        "            else:\n",
        "                target = reward\n",
        "                \n",
        "            # Construct the target vector as follows:\n",
        "            # 1. Use the current model to output the Q-value predictions\n",
        "            target_f = self.model.predict(state)\n",
        "            \n",
        "            # 2. Rewrite the chosen action value with the computed target\n",
        "            target_f[0][action] = target\n",
        "            \n",
        "            # 3. Use vectors in the objective computation\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "            \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    #\n",
        "    # Sets the target model parameters to the current model parameters\n",
        "    #\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "            \n",
        "    #\n",
        "    # Loads a saved model\n",
        "    #\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    #\n",
        "    # Saves parameters of a trained model\n",
        "    #\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbmA8lKDUHrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_frame(frame):\n",
        "\n",
        "    mspacman_color = np.array([210, 164, 74]).mean()\n",
        "    img = frame[1:176:2, ::2]    # Crop and downsize\n",
        "    img = img.mean(axis=2)       # Convert to greyscale\n",
        "    img[img==mspacman_color] = 0 # Improve contrast by making pacman white\n",
        "    img = (img - 128) / 128 - 1  # Normalize from -1 to 1.\n",
        "    \n",
        "    return np.expand_dims(img.reshape(88, 80, 1), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCFxjHaaULgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def  blend_images (images, blend):\n",
        "    avg_image = np.expand_dims(np.zeros((88, 80, 1), np.float64), axis=0)\n",
        "\n",
        "    for image in images:\n",
        "        avg_image += image\n",
        "        \n",
        "    if len(images) < blend:\n",
        "        return avg_image / len(images)\n",
        "    else:\n",
        "        return avg_image / blend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IqM_jW1UOhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('MsPacman-v0')\n",
        "state_size = (88, 80, 1)\n",
        "action_size = env.action_space.n\n",
        "agent = DDQN_Agent(state_size, action_size)\n",
        "#agent.load('models/')\n",
        "\n",
        "episodes = 5000\n",
        "batch_size = 32\n",
        "skip_start = 90  # MsPacman-v0 waits for 90 actions before the episode begins\n",
        "total_time = 0   # Counter for total number of steps taken\n",
        "all_rewards = 0  # Used to compute avg reward over time\n",
        "blend = 4        # Number of images to blend\n",
        "done = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKz7XbbGUZRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for e in range(episodes):\n",
        "    total_reward = 0\n",
        "    game_score = 0\n",
        "    state = process_frame(env.reset())\n",
        "    images = deque(maxlen=blend)  # Array of images to be blended\n",
        "    images.append(state)\n",
        "    \n",
        "    for skip in range(skip_start): # skip the start of each game\n",
        "        env.step(0)\n",
        "    \n",
        "    for time in range(20000):\n",
        "        env.render()\n",
        "        total_time += 1\n",
        "        \n",
        "        # Every update_rate timesteps we update the target network parameters\n",
        "        if total_time % agent.update_rate == 0:\n",
        "            agent.update_target_model()\n",
        "        \n",
        "        # Return the avg of the last 4 frames\n",
        "        state = blend_images(images, blend)\n",
        "        \n",
        "        # Transition Dynamics\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        game_score += reward\n",
        "        total_reward += reward\n",
        "        \n",
        "        # Return the avg of the last 4 frames\n",
        "        next_state = process_frame(next_state)\n",
        "        images.append(next_state)\n",
        "        next_state = blend_images(images, blend)\n",
        "        \n",
        "        # Store sequence in replay memory\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        \n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            all_rewards += game_score\n",
        "            \n",
        "            print(\"episode: {}/{}, game score: {}, reward: {}, avg reward: {}, time: {}, total time: {}\"\n",
        "                  .format(e+1, episodes, game_score, total_reward, all_rewards/(e+1), time, total_time))\n",
        "            \n",
        "            break\n",
        "            \n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}